{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Feature Engineering](#toc1_)    \n",
    "  - [데이터 합치기](#toc1_1_)    \n",
    "  - [필요없는 피처 제거](#toc1_2_)    \n",
    "  - [데이터 스케일링](#toc1_3_)    \n",
    "  - [데이터 나누기](#toc1_4_)    \n",
    "- [모델 훈련 및 성능 검증](#toc2_)    \n",
    "  - [test dataset 예측 결과](#toc2_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib\n",
    "\n",
    "train_path = './data/train/'\n",
    "test_path = './data/validation/'\n",
    "\n",
    "train_label = pd.read_csv(train_path + 'label/1.걸음걸이/training_label.csv')\n",
    "train = pd.read_csv(train_path + 'raw/train_sleep.csv')\n",
    "\n",
    "test_label = pd.read_csv(test_path + 'label/1.걸음걸이/val_label.csv')\n",
    "test = pd.read_csv(test_path + 'raw/val_sleep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {\n",
    "    'CN': 0, \n",
    "    'MCI': 1,\n",
    "    'Dem': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label['label'] = train_label['DIAG_NM'].map(label_encoder)\n",
    "test_label['label'] = test_label['DIAG_NM'].map(label_encoder)\n",
    "train_label.drop(columns=['DIAG_NM'], inplace=True)\n",
    "test_label.drop(columns=['DIAG_NM'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 과 합치기\n",
    "train = train.merge(train_label, left_on='EMAIL', right_on='SAMPLE_EMAIL')\n",
    "test = test.merge(test_label, left_on='EMAIL', right_on='SAMPLE_EMAIL')\n",
    "\n",
    "train.drop(columns=['SAMPLE_EMAIL'], inplace=True)\n",
    "test.drop(columns=['SAMPLE_EMAIL'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[데이터 합치기](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], axis=0)\n",
    "all_data = all_data.drop(['label'], axis=1) # 타겟값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EMAIL', 'sleep_awake', 'sleep_bedtime_end', 'sleep_bedtime_start',\n",
       "       'sleep_breath_average', 'sleep_deep', 'sleep_duration',\n",
       "       'sleep_efficiency', 'sleep_hr_5min', 'sleep_hr_average',\n",
       "       'sleep_hr_lowest', 'sleep_hypnogram_5min', 'sleep_is_longest',\n",
       "       'sleep_light', 'sleep_midpoint_at_delta', 'sleep_midpoint_time',\n",
       "       'sleep_onset_latency', 'sleep_period_id', 'sleep_rem', 'sleep_restless',\n",
       "       'sleep_rmssd', 'sleep_rmssd_5min', 'sleep_score',\n",
       "       'sleep_score_alignment', 'sleep_score_deep', 'sleep_score_disturbances',\n",
       "       'sleep_score_efficiency', 'sleep_score_latency', 'sleep_score_rem',\n",
       "       'sleep_score_total', 'sleep_temperature_delta',\n",
       "       'sleep_temperature_deviation', 'sleep_total',\n",
       "       'CONVERT(sleep_hr_5min USING utf8)',\n",
       "       'CONVERT(sleep_hypnogram_5min USING utf8)',\n",
       "       'CONVERT(sleep_rmssd_5min USING utf8)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[필요없는 피처 제거](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['sleep_hr_5min', 'sleep_rmssd_5min', 'sleep_hypnogram_5min',\n",
    "                 'sleep_period_id', 'sleep_is_longest', 'EMAIL', 'sleep_is_longest', \n",
    "                 'sleep_rmssd', 'sleep_score', 'sleep_score_alignment', 'sleep_temperature_delta',\n",
    "                 'sleep_temperature_deviation'] # drop 확정\n",
    "\n",
    "remaining_features = [feature for feature in all_features \n",
    "                      if feature not in drop_features]\n",
    "\n",
    "all_data = all_data[remaining_features]\n",
    "all_data = all_data.select_dtypes(include=[int, float]) # 숫자형 데이터만 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[데이터 스케일링](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_data = StandardScaler().fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[데이터 나누기](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train)\n",
    "\n",
    "X = all_data[:num_train]\n",
    "X_test = all_data[num_train:]\n",
    "\n",
    "y = train['label'].values\n",
    "y_test = test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[모델 훈련 및 성능 검증](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 24\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# shuffle: 훈련 데이터가 시계열 데이터가 아니라면 섞어주는 것이 좋음\n",
    "# 시계열 데이터는 순서가 중요해서 데이터를 섞으면 안도미\n",
    "folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "models = {\n",
    "    \"Lasso Regression\": LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=random_seed),\n",
    "    \"Support Vector Machine\": SVC(kernel='linear', probability=True, random_state=random_seed),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_seed),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=random_seed, force_col_wise=True) #force_col_wise param: warning 제거를 위한 추가  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## Model: Lasso Regression ########################################\n",
      "=== Average Classification Report ===\n",
      "              precision    recall  f1-score      support\n",
      "0              0.612399  0.939282  0.741402  1156.200000\n",
      "1              0.461160  0.094837  0.157230   670.600000\n",
      "2              0.461978  0.120839  0.190855   114.200000\n",
      "accuracy       0.599382  0.599382  0.599382     0.599382\n",
      "macro avg      0.511846  0.384986  0.363162  1941.000000\n",
      "weighted avg   0.551308  0.599382  0.507187  1941.000000\n",
      "Average Accuracy: 0.5994\n",
      "Average F1-score: 0.5072\n",
      "============================================================\n",
      "######################################## Model: Decision Tree ########################################\n",
      "=== Average Classification Report ===\n",
      "              precision    recall  f1-score      support\n",
      "0              0.732096  0.719771  0.725785  1156.200000\n",
      "1              0.551775  0.560090  0.555795   670.600000\n",
      "2              0.400932  0.430664  0.414080   114.200000\n",
      "accuracy       0.647604  0.647604  0.647604     0.647604\n",
      "macro avg      0.561601  0.570175  0.565220  1941.000000\n",
      "weighted avg   0.650318  0.647604  0.648724  1941.000000\n",
      "Average Accuracy: 0.6476\n",
      "Average F1-score: 0.6487\n",
      "============================================================\n",
      "######################################## Model: Support Vector Machine ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Average Classification Report ===\n",
      "              precision    recall  f1-score      support\n",
      "0              0.595672  1.000000  0.746610  1156.200000\n",
      "1              0.000000  0.000000  0.000000   670.600000\n",
      "2              0.000000  0.000000  0.000000   114.200000\n",
      "accuracy       0.595672  0.595672  0.595672     0.595672\n",
      "macro avg      0.198557  0.333333  0.248870  1941.000000\n",
      "weighted avg   0.354826  0.595672  0.444735  1941.000000\n",
      "Average Accuracy: 0.5957\n",
      "Average F1-score: 0.4447\n",
      "============================================================\n",
      "######################################## Model: Gradient Boosting ########################################\n",
      "=== Average Classification Report ===\n",
      "              precision    recall  f1-score      support\n",
      "0              0.698728  0.891367  0.783367  1156.200000\n",
      "1              0.680108  0.416644  0.516597   670.600000\n",
      "2              0.802444  0.385370  0.519300   114.200000\n",
      "accuracy       0.697579  0.697579  0.697579     0.697579\n",
      "macro avg      0.727093  0.564460  0.606421  1941.000000\n",
      "weighted avg   0.698400  0.697579  0.675660  1941.000000\n",
      "Average Accuracy: 0.6976\n",
      "Average F1-score: 0.6757\n",
      "============================================================\n",
      "######################################## Model: LightGBM ########################################\n",
      "[LightGBM] [Info] Total Bins 3085\n",
      "[LightGBM] [Info] Number of data points in the train set: 7764, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.518021\n",
      "[LightGBM] [Info] Start training from score -1.062935\n",
      "[LightGBM] [Info] Start training from score -2.832570\n",
      "[LightGBM] [Info] Total Bins 3087\n",
      "[LightGBM] [Info] Number of data points in the train set: 7764, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.518021\n",
      "[LightGBM] [Info] Start training from score -1.062935\n",
      "[LightGBM] [Info] Start training from score -2.832570\n",
      "[LightGBM] [Info] Total Bins 3089\n",
      "[LightGBM] [Info] Number of data points in the train set: 7764, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.518021\n",
      "[LightGBM] [Info] Start training from score -1.062935\n",
      "[LightGBM] [Info] Start training from score -2.832570\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 7764, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.518238\n",
      "[LightGBM] [Info] Start training from score -1.062562\n",
      "[LightGBM] [Info] Start training from score -2.832570\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 7764, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -0.518021\n",
      "[LightGBM] [Info] Start training from score -1.062562\n",
      "[LightGBM] [Info] Start training from score -2.834760\n",
      "=== Average Classification Report ===\n",
      "              precision    recall  f1-score      support\n",
      "0              0.764071  0.881681  0.818670  1156.200000\n",
      "1              0.728082  0.583663  0.647867   670.600000\n",
      "2              0.868350  0.525339  0.653111   114.200000\n",
      "accuracy       0.757754  0.757754  0.757754     0.757754\n",
      "macro avg      0.786834  0.663561  0.706549  1941.000000\n",
      "weighted avg   0.757776  0.757754  0.749920  1941.000000\n",
      "Average Accuracy: 0.7578\n",
      "Average F1-score: 0.7499\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "models_ = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print('#'*40, f\"Model: {name}\", '#'*40)\n",
    "    \n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "    class_report_list = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        # 데이터 분할\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # 모델 학습\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 모델 예측\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # 평가\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')  # 불균형 데이터 고려\n",
    "        acc_scores.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Classification Report 저장\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        class_report_list.append(pd.DataFrame(report).T)\n",
    "\n",
    "    # Classification Report 평균\n",
    "    avg_class_report = pd.concat(class_report_list).groupby(level=0).mean()\n",
    "\n",
    "    # Model 저장\n",
    "    models_[name] = model\n",
    "    \n",
    "    # Accuracy 및 F1-score 평균 출력\n",
    "    print(f\"=== Average Classification Report ===\")\n",
    "    print(avg_class_report)\n",
    "    print(f\"Average Accuracy: {np.mean(acc_scores):.4f}\")\n",
    "    print(f\"Average F1-score: {np.mean(f1_scores):.4f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[test dataset 예측 결과](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======================================== TEST SET EVALUATION ======================================== \n",
      "\n",
      "=== Evaluating Lasso Regression on Test Set ===\n",
      "Test Accuracy: 0.7510\n",
      "Test F1-score: 0.6922\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.796376  0.943763  0.863828  1956.000000\n",
      "1              0.033784  0.016234  0.021930   308.000000\n",
      "2              0.833333  0.046729  0.088496   214.000000\n",
      "accuracy       0.751009  0.751009  0.751009     0.751009\n",
      "macro avg      0.554498  0.335575  0.324751  2478.000000\n",
      "weighted avg   0.704782  0.751009  0.692227  2478.000000\n",
      "============================================================\n",
      "=== Evaluating Decision Tree on Test Set ===\n",
      "Test Accuracy: 0.5589\n",
      "Test F1-score: 0.6013\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.813151  0.638548  0.715349  1956.000000\n",
      "1              0.134503  0.373377  0.197764   308.000000\n",
      "2              0.241379  0.098131  0.139535   214.000000\n",
      "accuracy       0.558918  0.558918  0.558918     0.558918\n",
      "macro avg      0.396344  0.370019  0.350883  2478.000000\n",
      "weighted avg   0.679421  0.558918  0.601289  2478.000000\n",
      "============================================================\n",
      "=== Evaluating Support Vector Machine on Test Set ===\n",
      "Test Accuracy: 0.7893\n",
      "Test F1-score: 0.6964\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.789346  1.000000  0.882273  1956.000000\n",
      "1              0.000000  0.000000  0.000000   308.000000\n",
      "2              0.000000  0.000000  0.000000   214.000000\n",
      "accuracy       0.789346  0.789346  0.789346     0.789346\n",
      "macro avg      0.263115  0.333333  0.294091  2478.000000\n",
      "weighted avg   0.623067  0.789346  0.696419  2478.000000\n",
      "============================================================\n",
      "=== Evaluating Gradient Boosting on Test Set ===\n",
      "Test Accuracy: 0.6994\n",
      "Test F1-score: 0.6802\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.809179  0.856339  0.832091  1956.000000\n",
      "1              0.128141  0.165584  0.144476   308.000000\n",
      "2              0.700000  0.032710  0.062500   214.000000\n",
      "accuracy       0.699354  0.699354  0.699354     0.699354\n",
      "macro avg      0.545773  0.351545  0.346356  2478.000000\n",
      "weighted avg   0.715101  0.699354  0.680163  2478.000000\n",
      "============================================================\n",
      "=== Evaluating LightGBM on Test Set ===\n",
      "Test Accuracy: 0.6764\n",
      "Test F1-score: 0.6714\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.819028  0.809816  0.814396  1956.000000\n",
      "1              0.166045  0.288961  0.210900   308.000000\n",
      "2              0.375000  0.014019  0.027027   214.000000\n",
      "accuracy       0.676352  0.676352  0.676352     0.676352\n",
      "macro avg      0.453358  0.370932  0.350774  2478.000000\n",
      "weighted avg   0.699520  0.676352  0.671388  2478.000000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/anaconda3/envs/env2501/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test Set 성능 평가\n",
    "print(\"\\n\", \"=\"*40, \"TEST SET EVALUATION\", \"=\"*40, \"\\n\")\n",
    "\n",
    "for name, model in models_.items():\n",
    "    print(f\"=== Evaluating {name} on Test Set ===\")\n",
    "\n",
    "    # 테스트 데이터 예측\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # 테스트 데이터 평가\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')  # 불균형 고려\n",
    "    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")\n",
    "    print(f\"Test Classification Report:\\n\", pd.DataFrame(test_report).T)\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2501",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
